{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Statistics__ is the branch of mathematics dealing with the collection, analysis, interpretation,\n",
    "presentation, and organization of numerical data.\n",
    "\n",
    "Statistics are mainly classified into two subbranches:\n",
    "1. __Descriptive statistics__: These are used to summarize data, such as the mean,\n",
    "standard deviation for continuous data types (such as age), whereas frequency\n",
    "and percentage are useful for categorical data (such as gender).\n",
    "\n",
    "\n",
    "2. __Inferential statistics__: Many times, a collection of the entire data (also known as\n",
    "population in statistical methodology) is impossible, hence a subset of the data\n",
    "points is collected, also called a sample, and conclusions about the entire\n",
    "population will be drawn, which is known as inferential statistics. Inferences are\n",
    "drawn using hypothesis testing, the estimation of numerical characteristics, the\n",
    "correlation of relationships within data, and so on.\n",
    "\n",
    "__Machine learning__ is the branch of computer science that utilizes past experience to learn\n",
    "from and use its knowledge to make future decisions. Machine learning is at the\n",
    "intersection of computer science, engineering, and statistics. The goal of machine learning is\n",
    "to generalize a detectable pattern or to create an unknown rule from given examples\n",
    "\n",
    "1. __Supervised learning__: This is teaching machines to learn the relationship between\n",
    "other variables and a target variable. The major segments within\n",
    "supervised learning are as follows:\n",
    "    1. Classification problem\n",
    "    2. Regression problem\n",
    "    \n",
    "    \n",
    "2. __Unsupervised learning__: In unsupervised learning, algorithms learn by\n",
    "themselves without any supervision or without any target variable provided. It is\n",
    "a question of finding hidden patterns and relations in the given data. The\n",
    "categories in unsupervised learning are as follows:\n",
    "    1. Dimensionality reduction\n",
    "    2. Clustering\n",
    "    \n",
    "    \n",
    "3. __Reinforcement learning__: This allows the machine or agent to learn its behavior\n",
    "based on feedback from the environment. In reinforcement learning, the agent\n",
    "takes a series of decisive actions without supervision and, in the end, a reward\n",
    "will be given, either +1 or -1. Based on the final payoff/reward, the agent\n",
    "reevaluates its paths. Reinforcement learning problems are closer to the artificial\n",
    "intelligence methodology rather than frequently used machine learning\n",
    "algorithms.\n",
    "\n",
    "Difference between Statistics and ML:\n",
    "1. Relationships are formed in forms of mathematical equations in statistics whereas in ML it is formed in the form of rule-based programming. \n",
    "2. Statistical model predicts the output with Machine learning just predicts the output with accuracy of 85 percent and having 90 percent confidence about it. Machine learning just predicts the output with accuracy of 85 percent.\n",
    "3. Statistics  -  Data will be split into 70 percent - 30 percent to create training and testing data. Model developed on training data and tested on testing data. ML - Data will be split into 50 percent - 25 percent - 25 percent to create training, validation, andtesting data. Models developed on training and hyperparameters are tuned on validation data and finally get evaluated against test data.\n",
    "\n",
    "Steps in building ML Model:\n",
    "1. Collection of Data\n",
    "2. Data preparation and outlier treatment\n",
    "3. Data Analysis and Feature Engineering\n",
    "4. Train algorithm on training and validation data\n",
    "5. Test algorithm on test data\n",
    "6. Deploy algorithm\n",
    "\n",
    "__Statistics Fundamentals__\n",
    "1. __Population__: This is the totality, the complete list of observations, or all the data\n",
    "points about the subject under study. \n",
    "\n",
    "\n",
    "2. __Sample__:A sample is a subset of a population, usually a small portion of the\n",
    "population that is being analyzed.\n",
    "\n",
    "To draw inferences from a sample by validating a hypothesis it is necessary that the sample is random.\n",
    "\n",
    "3. __Parameter versus Statistic__: Any measure that is calculated on the population is a\n",
    "parameter, whereas on a sample it is called a statistic.\n",
    "\n",
    "\n",
    "4. __Mean__: Arithmetic average. The mean is sensitive to outliers in the data. An outlier is the value of a set or column that is highly deviant from the many other values in the same data; it usually has very high or low values.\n",
    "\n",
    "\n",
    "5. __Median__:This is the midpoint of the data, and is calculated by either arranging it in ascending or descending order. If there are N observations.\n",
    "\n",
    "\n",
    "6. __Mode__:This is the most repetitive data point in the data.\n",
    "\n",
    "<img src=\"images/mean_median_mode.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([4, 5, 1, 6, 8, 1, 3, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.555555555555555"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(data)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median = np.median(data)\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = stats.mode(data)\n",
    "mode[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. __Measure of Variation__:Dispersion is the variation in the data, and measures the inconsistencies in the value of variables in the data. \n",
    "\n",
    "\n",
    "8. __Range__:Difference between the maximum and minimum of the value.\n",
    "\n",
    "\n",
    "9. __Variance__: This is the mean of squared deviations from the mean. The dimension of variance is the square of the actual values. The reason to use denominator N-1 for a sample instead of N in the population is due the degree of freedom. 1 degree of freedom lost in a sample by the time of calculating variance is due to extraction of substitution of sample. \n",
    "\n",
    "\n",
    "10. __Standard Deviation__: This is the square root of variance. By applying the square root on variance, we measure the dispersion with respect to the original variable rather than square of the dimension. \n",
    "\n",
    "\n",
    "11. __Quantiles__:These are identical fragments of the data. Quantiles cover percentiles, deciles, quartiles, and so on. These measures are calculated after arranging the data in ascending order\n",
    "    1. __Percentile__:This is the percentage of data points below the value of the original whole data. The median is the 50 th percentile, as the number of data points below the median is about 50 percent of the data.\n",
    "    2. __Decile__: This is 10th percentile, which means the number of data points below the decile is 10 percent of the whole data.\n",
    "    3. __Quartile__: This is one-fourth of the data, and also is the 25 percentile. The first quartile is 25 percent of the data, the second quartile is 50 percent of the data, the third quartile is 75 percent of the data. The second quartile is also known as the median or 50 th percentile or 5 th decile.\n",
    "    4. __Interquartile Range__: This is the difference between the third quartile and first quartile. It is effective in identifying outliers in data. The interquartile range describes the middle 50 percent of the data points.\n",
    "    \n",
    "    <img src=\"images/quantile.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import variance, stdev\n",
    "game_points = np.array([35, 46, 72, 38, 81, 41, 57, 93, 17, 33, 61, 75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variance\n",
    "variance_data = variance(game_points)\n",
    "variance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.825424421026653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Deviation\n",
    "standard_dev = stdev(game_points)\n",
    "standard_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Range\n",
    "range_data = np.max(game_points, axis=0) - np.min(game_points, axis=0)\n",
    "range_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % : 33.199999999999996\n",
      "20 % : 35.6\n",
      "30 % : 38.900000000000006\n",
      "40 % : 43.0\n",
      "50 % : 51.5\n",
      "60 % : 59.4\n",
      "70 % : 68.69999999999999\n",
      "80 % : 74.4\n",
      "90 % : 80.4\n"
     ]
    }
   ],
   "source": [
    "# Quantile\n",
    "for val in [10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
    "    quant = np.percentile(game_points, val)\n",
    "    print(val, '% :', quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. __Hypothesis Testing__: This is the process of making inferences about the overall population by conducting some statistical tests on a sample. Null and alternate hypotheses are ways to validate whether an assumption is statistically significant or not.\n",
    "\n",
    "A null hypothesis, proposes that no significant difference exists in a set of given observations\n",
    "\n",
    "13. __P-Value__: The probability of obtaining a test statistic result is at least as extreme as the one that was actually observed, assuming that the null hypothesis is true (usually in modeling, against each independent variable, a p-value less than 0.05 is considered significant and greater than 0.05 is considered insignificant; nonetheless, these values and definitions may change with respect to context).\n",
    "\n",
    "P value less than 0.05 means both claimed values and distribution mean values are significantly different, hence we can reject null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Steps involved in Hypothesis Testing__\n",
    "1. Assume a null hypothesis (usually no difference, no significance, and so on; a null hypothesis always tries to assume that there is no anomaly pattern and is always homogeneous, and so on).\n",
    "2. Collect the sample.\n",
    "3. Calculate test statistics from the sample in order to verify whether the hypothesis is statistically significant or not.\n",
    "4. Decide either to accept or reject the null hypothesis based on the test statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Statistic and Critical Value__\n",
    "In hypothesis testing, a critical value is a point on test distribution that is compared to the test statistic to determine whether to reject null hypothesis. If absolute value of test statistic is greater than critical value, then it would be correct to declare statistical significance and reject null hypothesis. Critical values correspond to alpha, so their values become fixed when we chosse the test's alpha. \n",
    "\n",
    "The __critical values__ are the boundaries of the critical region. If the test is one-sided (like a χ2 test or a one-sided t-test) then there will be just one critical value, but in other cases (like a two-sided t-test) there will be two”.\n",
    "\n",
    "A __critical value__ is a point (or points) on the scale of the test statistic beyond which we reject the null hypothesis, and, is derived from the level of significance α of the test. Critical value can tell us, what is the probability of two sample means belonging to the same distribution. Higher, the critical value means lower the probability of two samples belonging to same distribution. The general critical value for a two-tailed test is 1.96, which is based on the fact that 95% of the area of a normal distribution is within 1.96 standard deviations of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.381780460041329"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "xbar = 990\n",
    "mu0 = 1000\n",
    "s = 12.5\n",
    "n = 30\n",
    "# Test Statistic\n",
    "t_smple = (xbar-mu0)/(s/np.sqrt(float(n)))\n",
    "t_smple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6991270265334977"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Critical Value\n",
    "alpha = 0.05\n",
    "t_alpha = stats.t.ppf(alpha, n-1)\n",
    "t_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.035025729010886e-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P Value\n",
    "p_val = stats.t.sf(np.abs(t_smple), n-1)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. __Type I and Type II Error__: Hypothesis testing is usually done on the samples rather\n",
    "than the entire population, due to the practical constraints of available resources\n",
    "to collect all the available data. However, performing inferences about the\n",
    "population from samples comes with its own costs, such as rejecting good results\n",
    "or accepting false results, not to mention separately, when increases in sample\n",
    "size lead to minimizing type I and II errors:\n",
    "    1. __Type I error__: Rejecting a null hypothesis when it is true\n",
    "    2. __Type II error__: Accepting a null hypothesis when it is false\n",
    "    \n",
    "    \n",
    "15. __Normal Distribution__:This is very important in statistics because of the central limit theorem, which states that the population of all possible samples of size n from a population with mean μ and variance σ2 approaches a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920245398773006"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Z-Score\n",
    "xbar = 67\n",
    "mu0 = 52\n",
    "s = 16.3\n",
    "z = (xbar-mu0)/s\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.872226751475175"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability Under Curve \n",
    "p_val = 1 - stats.norm.cdf(z)\n",
    "p_val*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. __Chi-square__:This test of independence is one of the most basic and common hypothesis tests in the statistical analysis of categorical data. Given two categorical random variables X and Y, the chi-square test of independence determines whether or not there exists a statistical dependence between them.\n",
    "\n",
    "The test is usually performed by calculating χ2 from the data and χ2 with\n",
    "(m-1, n-1) degrees from the table. A decision is made as to whether both\n",
    "variables are independent based on the actual value and table value,\n",
    "whichever is higher.\n",
    "\n",
    "<img src=\"images/chi-square.png\">\n",
    "\n",
    "The chi2_contingency function in the stats package uses the observed table and subsequently calculates its expected table, followed by calculating the p-value in order to check whether two variables are dependent or not. If p-value < 0.05, there is a strong dependency between two variables, whereas if p-value > 0.05, there is no dependency between the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Wr.Hnd</th>\n",
       "      <th>NW.Hnd</th>\n",
       "      <th>W.Hnd</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Pulse</th>\n",
       "      <th>Clap</th>\n",
       "      <th>Exer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Height</th>\n",
       "      <th>M.I</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>92.0</td>\n",
       "      <td>Left</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>173.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>18.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>19.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>Left</td>\n",
       "      <td>R on L</td>\n",
       "      <td>104.0</td>\n",
       "      <td>Left</td>\n",
       "      <td>None</td>\n",
       "      <td>Regul</td>\n",
       "      <td>177.8</td>\n",
       "      <td>Imperial</td>\n",
       "      <td>17.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>Right</td>\n",
       "      <td>L on R</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Neither</td>\n",
       "      <td>None</td>\n",
       "      <td>Occas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.9</td>\n",
       "      <td>Right</td>\n",
       "      <td>R on L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neither</td>\n",
       "      <td>None</td>\n",
       "      <td>Never</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>20.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Neither</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>Some</td>\n",
       "      <td>Never</td>\n",
       "      <td>165.0</td>\n",
       "      <td>Metric</td>\n",
       "      <td>23.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex  Wr.Hnd  NW.Hnd  W.Hnd     Fold  Pulse     Clap  Exer  Smoke  \\\n",
       "0  Female    18.5    18.0  Right   R on L   92.0     Left  Some  Never   \n",
       "1    Male    19.5    20.5   Left   R on L  104.0     Left  None  Regul   \n",
       "2    Male    18.0    13.3  Right   L on R   87.0  Neither  None  Occas   \n",
       "3    Male    18.8    18.9  Right   R on L    NaN  Neither  None  Never   \n",
       "4    Male    20.0    20.0  Right  Neither   35.0    Right  Some  Never   \n",
       "\n",
       "   Height       M.I     Age  \n",
       "0   173.0    Metric  18.250  \n",
       "1   177.8  Imperial  17.583  \n",
       "2     NaN       NaN  16.917  \n",
       "3   160.0    Metric  20.333  \n",
       "4   165.0    Metric  23.667  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from scipy import stats\n",
    "\n",
    "survey = pd.read_csv('Data Files/survey.csv')\n",
    "survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Exer</th>\n",
       "      <th>Freq</th>\n",
       "      <th>None</th>\n",
       "      <th>Some</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Heavy</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Never</th>\n",
       "      <td>87</td>\n",
       "      <td>18</td>\n",
       "      <td>84</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occas</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regul</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>115</td>\n",
       "      <td>23</td>\n",
       "      <td>98</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Exer   Freq  None  Some  All\n",
       "Smoke                       \n",
       "Heavy     7     1     3   11\n",
       "Never    87    18    84  189\n",
       "Occas    12     3     4   19\n",
       "Regul     9     1     7   17\n",
       "All     115    23    98  236"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_tab = pd.crosstab(survey['Smoke'], survey['Exer'], margins=True)\n",
    "survey_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Exer</th>\n",
       "      <th>Freq</th>\n",
       "      <th>None</th>\n",
       "      <th>Some</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Heavy</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Never</th>\n",
       "      <td>87</td>\n",
       "      <td>18</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occas</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regul</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Exer   Freq  None  Some\n",
       "Smoke                  \n",
       "Heavy     7     1     3\n",
       "Never    87    18    84\n",
       "Occas    12     3     4\n",
       "Regul     9     1     7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed = survey_tab.iloc[0:4, 0:3]\n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.483"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contg = stats.chi2_contingency(observed=observed)\n",
    "p_value = round(contg[1], 3)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is 0.483 , which means there is no dependency between the smoking habit and exercise behavior.\n",
    "\n",
    "Stats.Chi2_Contingency returns following:\n",
    "1. Test Statistics\n",
    "2. P Value\n",
    "3. Degree of Freedom "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. __ANOVA__:Analyzing variance tests the hypothesis that the means of two or more populations are equal. ANOVAs assess the importance of one or more factors by comparing the response variable means at the different factor levels. The null hypothesis states that all population means are equal while the alternative hypothesis states that at least one is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fertilizer1</th>\n",
       "      <th>fertilizer2</th>\n",
       "      <th>fertilizer3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>58</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fertilizer1  fertilizer2  fertilizer3\n",
       "0           62           54           48\n",
       "1           62           56           62\n",
       "2           90           58           92\n",
       "3           42           36           96\n",
       "4           84           72           92"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "data = pd.read_csv('Data Files/fetilizers.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic : 3.66 , p-value: 0.051\n"
     ]
    }
   ],
   "source": [
    "# One-Way ANOVA\n",
    "one_way_anova = stats.f_oneway(data['fertilizer1'], data['fertilizer2'], data['fertilizer3'])\n",
    "print (\"Statistic :\", round(one_way_anova[0],2),\", p-value:\",round(one_way_anova[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value did come equal to 0.05, hence we accept the null hypothesis that the mean crop yields of the fertilizers are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. __Confusion Matrix__:This is the matrix of the actual versus the predicted. The table contains following:\n",
    "    1. __True positives (TPs)__: True positives are cases when we predict the outcome(class) and it is correct.\n",
    "    2. __True negatives (TNs)__: Cases when we predict the outcome (class) and the class is actually not there.\n",
    "    3. __False positives (FPs)__: When we predict the outcome as yes when the outcome actually does not have it. FPs are also considered to be type I errors.\n",
    "    4. __False negatives (FNs)__: When we predict the outcome as no when the outcome actually does have it. FNs are also considered to be type II errors.\n",
    "    5. __Precision (P)__: When yes is predicted, how often is it correct? (TP/TP+FP)\n",
    "    6. __Recall (R)/sensitivity/true positive rate__: Among the actual yeses, what fraction was predicted as yes? (TP/TP+FN)\n",
    "    \n",
    "    \n",
    "19. __F1 Score (F1)__:This is the harmonic mean of the precision and recall. Multiplying the constant of 2 scales the score to 1 when both precision and recall are 1. \n",
    "\n",
    "\n",
    "20. __Specificity__:Among the actual nos, what fraction was predicted as no? Also equivalent to 1- false positive rate: (TN/TN+FP)\n",
    "\n",
    "\n",
    "21. __Area Under Curve (ROC)__:Receiver operating characteristic curve is used to plot between true positive rate (TPR) and false positive rate (FPR), also known as a sensitivity and 1- specificity graph.\n",
    "\n",
    "<img src=\"images/roc.png\">\n",
    "\n",
    "\n",
    "    Area under curve is utilized for setting the threshold of cut-off\n",
    "    probability to classify the predicted probability into various classes;\n",
    "    we will be covering how this method works in upcoming chapters.\n",
    "    \n",
    "\n",
    "22. __Observation and performance window__:In statistical modeling, the model tries to predict the event in advance rather than at the moment, so that some buffer time will exist to work on corrective actions. For example, a question from a credit card company would be, for example, what is the probability that a particular customer will default in the coming 12-month period? So that I can call him and offer any discounts or develop my collection strategies accordingly.\n",
    "    In order to answer this question, a probability of default model (or behavioral scorecard in technical terms) needs to be developed by using independent variables from the past 24 months and a dependent variable from the next 12 months. After preparing data with X and Y variables, it will be split into 70 percent - 30 percent as train and test data randomly; this method is called in-time validation as both train and test samples are from the same time period.\n",
    "    \n",
    "\n",
    "23. __In-time and out-of-time validation__: In-time validation implies obtaining both a training and testing dataset from the same period of time, whereas out-of-time validation implies training and testing datasets drawn from different time periods. Usually, the model performs worse in out-of-time validation rather than in-time due to the obvious reason that the characteristics of the train and test datasets might differ.\n",
    "\n",
    "\n",
    "24. __R-squared (coefficient of determination)__:This is the measure of the percentage of the response variable variation that is explained by a model. It also a measure of how well the model minimizes error compared with just utilizing the mean as an estimate. In some extreme cases, R-squared can have a value less than zero also, which means the predicted values from the model perform worse than just taking the simple mean as a prediction for all the observations. We will study this parameter in detail in upcoming chapters.\n",
    "\n",
    "<img src=\"images/r_square.png\">\n",
    "\n",
    "SST - Sum of Squares of Total \n",
    "SSE - Sum of Squares of Error\n",
    "\n",
    "The difference between SST and SSE is the improvement in prediction from the regression model, compared to the mean model. Dividing that difference by SST gives R-squared. It is the proportional improvement in prediction from the regression model, compared to the mean model. It indicates the goodness of fit of the model.\n",
    "\n",
    "R-squared has the useful property that its scale is intuitive: it ranges from zero to one, with zero indicating that the proposed model does not improve prediction over the mean model, and one indicating perfect prediction. Improvement in the regression model results in proportional increases in R-squared.\n",
    "\n",
    "One pitfall of R-squared is that it can only increase as predictors are added to the regression model. This increase is artificial when predictors are not actually improving the model’s fit. To remedy this, a related statistic, Adjusted R-squared, incorporates the model’s degrees of freedom. Adjusted R-squared will decrease as predictors are added if the increase in model fit does not make up for the loss of degrees of freedom. Likewise, it will increase as predictors are added if the increase in model fit is worthwhile. Adjusted R-squared should always be used with models with more than one predictor variable. It is interpreted as the proportion of total variance that is explained by the model.\n",
    "\n",
    "There are situations in which a high R-squared is not necessary or relevant. When the interest is in the relationship between variables, not in prediction, the R-square is less important. An example is a study on how religiosity affects health outcomes. A good result is a reliable relationship between religiosity and health. No one would expect that religion explains a high percentage of the variation in health, as health is affected by many other factors. Even if the model accounts for other variables known to affect health, such as income and age, an R-squared in the range of 0.10 to 0.15 is reasonable.\n",
    "\n",
    "\n",
    "25. __Adjusted R-square__:The explanation of the adjusted R-squared statistic is almost the same as R-squared but it penalizes the R-squared value if extra variables without a strong correlation are included in the model.\n",
    "\n",
    "<img src=\"images/adjusted_r_square.png\">\n",
    "\n",
    "    Here, R2 = sample R-squared value, n = sample size, k = number of predictors (or) variables. Adjusted R-squared value is the key metric in evaluating the quality of linear regressions. Any linear regression model having the value of R2 adjusted >= 0.7 is considered as a good enough model to implement.\n",
    "\n",
    "\n",
    "26. __The F-test__: It evaluates the null hypothesis that all regression coefficients are equal to zero versus the alternative that at least one is not. An equivalent null hypothesis is that R-squared equals zero. A significant F-test indicates that the observed R-squared is reliable and is not a spurious result of oddities in the data set. Thus the F-test determines whether the proposed relationship between the response variable and the set of predictors is statistically reliable and can be useful when the research objective is either prediction or explanation.\n",
    "\n",
    "\n",
    "27. __RMSE (Root Mean Square Error)__: It is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.\n",
    "\n",
    "\n",
    "28. __Maximum Likelihood Estimation (MLE)__:This is estimating the parameter values of a statistical model (logistic regression, to be precise) by finding the parameter values that maximize the likelihood of making the observations. \n",
    "\n",
    "\n",
    "29. __Bias and Variance Trade off__: Every model has both bias and variance error components in addition to white noise. Bias and variance are inversely related to each other; while trying to reduce one component, the other component of the model will increase. The true art lies in creating a good fit by balancing both. The ideal model will have both low bias and low variance. Errors from the bias component come from erroneous assumptions in the underlying learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs; this phenomenon causes an underfitting problem. On the other hand, errors from the variance component come from sensitivity to change in the fit of the model, even a small change in training data; high variance can cause an overfitting problem.\n",
    "\n",
    "    An example of a high bias model is logistic or linear regression, in which the fit of the model is merely a straight line and may have a high error component due to the fact that a linear model could not approximate underlying data well. An example of a high variance model is a decision tree, in which the model may create too\n",
    "    much wiggly curve as a fit, in which even a small change in training data will cause a\n",
    "    drastic change in the fit of the curve. At the moment, state-of-the-art models are utilizing high variance models such as decision trees and performing ensemble on top of them to reduce the errors caused by high variance and at the same time not compromising on increases in errors due to the bias component.\n",
    "    The best example of this category is random forest, in which many decision trees will be grown independently and ensemble in order to come up with the best fit.\n",
    "    \n",
    "    \n",
    "30. __Convex and Non-Convex Function__: Convex functions are functions in which a line drawn between any two random points on the function also lies within the function, whereas this isn't true for non-convex functions. It is important to know whether the function is convex or non-convex due to the fact that in convex functions, the local optimum is also the global optimum, whereas for non-convex functions, the local optimum does not guarantee the global optimum.\n",
    "\n",
    "\n",
    "31. __Gradient descent__: This is a way to minimize the objective function J(Θ) d parameterized by the model's parameter Θ ε R by updating the parameters in the opposite direction to the gradient of the objective function with respect to the parameters. The learning rate determines the size of steps taken to reach the minimum.\n",
    "\n",
    "\n",
    "32. __Full batch gradient descent (all training observations considered in each and every iteration)__: In full batch gradient descent, all the observations are considered for each and every iteration; this methodology takes a lot of memory and will be slow as well. Also, in practice, we do not need to have all the observations to update the weights. Nonetheless, this method provides the best way of updating parameters with less noise at the expense of huge computation.\n",
    "\n",
    "\n",
    "33. __Stochastic gradient descent (one observation per iteration)__: This method updates weights by taking one observation at each stage of iteration. This method provides the quickest way of traversing weights; however, a lot of noise is involved while converging.\n",
    "\n",
    "\n",
    "34. __Mini batch gradient descent (about 30 training observations or more for each and every iteration)__: This is a trade-off between huge computational costs and a quick method of updating weights. In this method, at each iteration, about 30 observations will be selected at random and gradients calculated to update the model weights. Here, a question many can ask is, why the minimum 30 and not any other number? If we look into statistical basics, 30 observations required to be considering in order approximating sample as a population. However, even 40, 50, and so on will also do well in batch size selection. Nonetheless, a practitioner needs to change the batch size and verify the results, to determine at what value the model is producing the optimum results.\n",
    "\n",
    "\n",
    "35. __Cross Validation__: Cross-validation is another way of ensuring robustness in the model at the expense of computation. In the ordinary modeling methodology, a model is developed on train data and evaluated on test data. In some extreme cases, train and test might not have been homogeneously selected and some unseen extreme cases might appear in the test data, which will drag down the performance of the model. On the other hand, in cross-validation methodology, data was divided into equal parts and training performed on all the other parts of the data except one part, on which performance will be evaluated. This process repeated as many parts user has chosen.\n",
    "\n",
    "    Example: In five-fold cross-validation, data will be divided into five parts, subsequently\n",
    "    trained on four parts of the data, and tested on the one part of the data. This process will\n",
    "    run five times, in order to cover all points in the data. Finally, the error calculated will be\n",
    "    the average of all the errors, \n",
    "    <img src=\"images/cross_validation.png\">\n",
    "    \n",
    "    \n",
    "36. __Grid Search__:Grid search in machine learning is a popular way to tune the hyperparameters of the model in order to find the best combination for determining the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1549</th>\n",
       "      <th>1550</th>\n",
       "      <th>1551</th>\n",
       "      <th>1552</th>\n",
       "      <th>1553</th>\n",
       "      <th>1554</th>\n",
       "      <th>1555</th>\n",
       "      <th>1556</th>\n",
       "      <th>1557</th>\n",
       "      <th>1558</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>468</td>\n",
       "      <td>8.2105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>230</td>\n",
       "      <td>6.9696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>468</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>468</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1       2     3     4     5     6     7     8     9     ...  1549  \\\n",
       "0   125   125  1.0000     1     0     0     0     0     0     0  ...     0   \n",
       "1    57   468  8.2105     1     0     0     0     0     0     0  ...     0   \n",
       "2    33   230  6.9696     1     0     0     0     0     0     0  ...     0   \n",
       "3    60   468  7.8000     1     0     0     0     0     0     0  ...     0   \n",
       "4    60   468  7.8000     1     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   1550  1551  1552  1553  1554  1555  1556  1557  1558  \n",
       "0     0     0     0     0     0     0     0     0     1  \n",
       "1     0     0     0     0     0     0     0     0     1  \n",
       "2     0     0     0     0     0     0     0     0     1  \n",
       "3     0     0     0     0     0     0     0     0     1  \n",
       "4     0     0     0     0     0     0     0     0     1  \n",
       "\n",
       "[5 rows x 1559 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pd.read_csv(\"Data Files/ad.csv\",header=None)\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = set(input_data.columns.values)\n",
    "y = input_data[len(input_data.columns.values)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns.remove(len(input_data.columns.values)-1)\n",
    "X = input_data[list(X_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('clf', DecisionTreeClassifier(criterion='entropy'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'clf__max_depth': (50,100,150),\n",
    "            'clf__min_samples_split': (2, 3),\n",
    "            'clf__min_samples_leaf': (1, 2, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepshikha/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('clf',\n",
       "                                        DecisionTreeClassifier(class_weight=None,\n",
       "                                                               criterion='entropy',\n",
       "                                                               max_depth=None,\n",
       "                                                               max_features=None,\n",
       "                                                               max_leaf_nodes=None,\n",
       "                                                               min_impurity_decrease=0.0,\n",
       "                                                               min_impurity_split=None,\n",
       "                                                               min_samples_leaf=1,\n",
       "                                                               min_samples_split=2,\n",
       "                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                               presort=False,\n",
       "                                                               random_state=None,\n",
       "                                                               splitter='best'))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__max_depth': (50, 100, 150),\n",
       "                         'clf__min_samples_leaf': (1, 2, 3),\n",
       "                         'clf__min_samples_split': (2, 3)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score: \n",
      " 0.966884531590414\n",
      "\n",
      " Best parameters set: \n",
      "\n",
      "\tclf__max_depth: 100\n",
      "\n",
      " Confusion Matrix on Test data\n",
      " [[814  19]\n",
      " [ 16 135]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.9644308943089431\n",
      "\n",
      "Precision Recall f1 table \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       833\n",
      "           1       0.88      0.89      0.89       151\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.93      0.94      0.93       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n",
      "\tclf__min_samples_leaf: 1\n",
      "\n",
      " Confusion Matrix on Test data\n",
      " [[814  19]\n",
      " [ 16 135]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.9644308943089431\n",
      "\n",
      "Precision Recall f1 table \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       833\n",
      "           1       0.88      0.89      0.89       151\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.93      0.94      0.93       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n",
      "\tclf__min_samples_split: 2\n",
      "\n",
      " Confusion Matrix on Test data\n",
      " [[814  19]\n",
      " [ 16 135]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.9644308943089431\n",
      "\n",
      "Precision Recall f1 table \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       833\n",
      "           1       0.88      0.89      0.89       151\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.93      0.94      0.93       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('\\n Best score: \\n', grid_search.best_score_)\n",
    "print ('\\n Best parameters set: \\n')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    print (\"\\n Confusion Matrix on Test data\\n\",confusion_matrix(y_test,y_pred))\n",
    "    print (\"\\n Test Accuracy \\n\",accuracy_score(y_test,y_pred))\n",
    "    print (\"\\nPrecision Recall f1 table \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Logistic regression__: This is the problem in which outcomes are discrete classes rather than continuous values. For example, a customer will arrive or not, he will purchase the product or not, and so on. In statistical methodology, it uses the maximum likelihood method to calculate the parameter of individual variables. In contrast, in machine learning methodology, log loss will be minimized with respect to β coefficients (also known as weights). Logistic regression has a high bias and a low variance error.\n",
    "\n",
    "\n",
    "2. __Linear regression__: This is used for the prediction of continuous variables such as customer income and so on. It utilizes error minimization to fit the best possible line in statistical methodology. However, in machine learning methodology, squared loss will be minimized with respect to β coefficients. Linear regression also has a high bias and a low variance error.\n",
    "\n",
    "\n",
    "3. __Lasso and ridge regression__: This uses regularization to control overfitting issues by applying a penalty on coefficients. In ridge regression, a penalty is applied on the sum of squares of coefficients, whereas in lasso, a penalty is applied on the absolute values of the coefficients. The penalty can be tuned in order to change the dynamics of the model fit. Ridge regression tries to minimize the magnitude of coefficients, whereas lasso tries to eliminate them.\n",
    "\n",
    "\n",
    "4. __Decision trees__: Recursive binary splitting is applied to split the classes at each level to classify observations to their purest class. The classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class. Decision trees have an overfitting problem due to their high variance in a way to fit; pruning is applied to reduce the overfitting problem by growing the tree completely. Decision trees have low a bias and a high variance error.\n",
    "\n",
    "\n",
    "5. __Bagging__: This is an ensemble technique applied on decision trees in order to minimize the variance error and at the same time not increase the error component due to bias. In bagging, various samples are selected with a subsample of observations and all variables (columns), subsequently fit individual decision trees independently on each sample and later ensemble the results by taking the maximum vote (in regression cases, the mean of outcomes calculated).\n",
    "\n",
    "\n",
    "6. __Random forest__: This is similar to bagging except for one difference. In bagging, all the variables/columns are selected for each sample, whereas in random forest a few subcolumns are selected. The reason behind the selection of a few variables rather than all was that during each independent tree sampled, significant variables always came first in the top layer of splitting which makes all the trees look more or less similar and defies the sole purpose of ensemble: that it works better on diversified and independent individual models rather than correlated individual models. Random forest has both low bias and variance errors.\n",
    "\n",
    "7. __Boosting__: This is a sequential algorithm that applies on weak classifiers such as a decision stump (a one-level decision tree or a tree with one root node and two terminal nodes) to create a strong classifier by ensembling the results. The algorithm starts with equal weights assigned to all the observations, followed by subsequent iterations where more focus was given to misclassified observations by increasing the weight of misclassified observations and decreasing the weight of properly classified observations. In the end, all the individual classifiers were combined to create a strong classifier. Boosting might have an overfitting problem, but by carefully tuning the parameters, we can obtain the best of the self machine learning model.\n",
    "\n",
    "\n",
    "8. __Support vector machines (SVMs)__: This maximizes the margin between classes by fitting the widest possible hyperplane between them. In the case of non-linearly separable classes, it uses kernels to move observations into higher-dimensional space and then separates them linearly with the hyperplane there.\n",
    "\n",
    "\n",
    "9. __Support vector machines (SVMs)__: This maximizes the margin between classes by fitting the widest possible hyperplane between them. In the case of non-linearly separable classes, it uses kernels to move observations into higher-dimensional space and then separates them linearly with the hyperplane there.\n",
    "\n",
    "\n",
    "10. __Principal component analysis (PCA)__: This is a dimensionality reduction technique in which principal components are calculated in place of the original variable. Principal components are determined where the variance in data is maximum; subsequently, the top n components will be taken by covering about 80 percent of variance and will be used in further modeling processes, or exploratory analysis will be performed as unsupervised learning.\n",
    "\n",
    "\n",
    "11. __K-means clustering__: This is an unsupervised algorithm that is mainly utilized for segmentation exercise. K-means clustering classifies the given data into k clusters in such a way that, within the cluster, variation is minimal and across the cluster, variation is maximal.\n",
    "\n",
    "\n",
    "12. __Markov decision process (MDP)__: In reinforcement learning, MDP is a mathematical framework for modeling decision-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system's state. The objective is to control the system in such a way that the agent's total payoff is maximized.\n",
    "\n",
    "\n",
    "13. __Monte Carlo method__: Monte Carlo methods do not require complete knowledge of the environment, in contrast with MDP. Monte Carlo methods require only experience, which is obtained by sample sequences of states, actions, and rewards from actual or simulated interaction with the environment. Monte Carlo methods explore the space until the final outcome of a chosen sample sequences and update estimates accordingly.\n",
    "\n",
    "\n",
    "14. __Temporal difference learning__: This is a core theme in reinforcement learning. Temporal difference is a combination of both Monte Carlo and dynamic programming ideas. Similar to Monte Carlo, temporal difference methods can learn directly from raw experience without a model of the environment's dynamics. Like dynamic programming, temporal difference methods update estimates based in part on other learned estimates, without waiting for a final outcome. Temporal difference is the best of both worlds and is most commonly used in games such as AlphaGo and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
